{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "This is the code which is responsibile for calling the LLAMA3 LLM for inferring for each post: the problem, the causes and the possible solutions (if any) to the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries to be imported\n",
    "import json\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing in a dedicated JSON file\n",
    "def write_json(file_path, dict):\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(dict, file, indent=4, separators=(',',': '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"I provide you a JSON file that contains a post from Reddit containing problems and solutions to them.\"\n",
    "prompt += \" Identify for this post: causes of the problem, the problem and solutions to the problem.\"\n",
    "prompt += \" Reply in this way: Problem: <p1,...,pn>; Causes: <c1,.., cn>; Solutions: <s1,...,sn>. The reply must contain only short keywords.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def count_tokens(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_output_parser(text):\n",
    "    result = {}\n",
    "    # Using regex to match the three sections\n",
    "    problem_match = re.search(r'Problem:\\s*(.*?);', text)\n",
    "    causes_match = re.search(r'Causes:\\s*(.*?);', text)\n",
    "    solutions_match = re.search(r'Solutions:\\s*(.*)', text)\n",
    "\n",
    "    if problem_match:\n",
    "        result['Problem'] = problem_match.group(1).strip()\n",
    "\n",
    "    if causes_match:\n",
    "        result['Causes'] = [cause.strip() for cause in causes_match.group(1).split(',')]\n",
    "\n",
    "    if solutions_match:\n",
    "        result['Solutions'] = [solution.strip() for solution in solutions_match.group(1).split(',')]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4773"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scan the whole set of documents and find the maxium number of tokens\n",
    "\n",
    "# Here I'm opening the file one by one from the collection directory and we add the content as element in the data list\n",
    "def load_json_files(directory):\n",
    "    data = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".json\"):\n",
    "            with open(os.path.join(directory, filename), 'r', encoding='utf-8') as f:\n",
    "                data.extend(json.load(f))\n",
    "    return data\n",
    "\n",
    "# Function calling: loading all the post in memory\n",
    "dir = \"./dataCollection\"\n",
    "posts = load_json_files(dir)\n",
    "\n",
    "# Iterating each document and converting into a plain string\n",
    "# Find the max number of tokens\n",
    "token_nums = []\n",
    "for i, document in enumerate(posts):\n",
    "    document_string = json.dumps(document, separators=(',', ':'), ensure_ascii=False) # This line will both convert the document into a string and removes commas\n",
    "    token_nums.append(count_tokens(prompt + \" \" + document_string))\n",
    "max_token_num = max(token_nums)\n",
    "max_token_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this analysis, I conclude that the maximum possible number of tokens for my dataset it's equal to 4761. So, I'll use a \"Context Lenght\" equal to 5000 in my local LLM-LLAMA3 server instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "sys_prompt = \"Reply directly according to what instructed. Don't take initiatives like\"\n",
    "sys_prompt += \"putting -, * or newline characters.\"\n",
    "# Using LLAMA3 LLM\n",
    "def prompt_issueing_llm(prompt, document_string):\n",
    "    prompt_content = prompt + \" \" + document_string\n",
    "    # Pointing to the local server\n",
    "    client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "    completion = client.chat.completions.create(\n",
    "        model = \"mstudio-community/Meta-Llama-3-8B-Instruct-GGUF\",\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": sys_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt_content}\n",
    "        ],\n",
    "        temperature = 0.7\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating each document and converting into a plain string\n",
    "def graph_inference(start = 1, end=len(posts)):\n",
    "   file_count = start\n",
    "   count = 0\n",
    "   for i, document in enumerate(posts):\n",
    "      if(count < start - 1):\n",
    "         count += 1\n",
    "         continue\n",
    "      if(count >= end):\n",
    "         break\n",
    "      document_string = json.dumps(document, separators=(',', ':'), ensure_ascii=False) # This line will both convert the document into a string and removes commas\n",
    "      llm_response = prompt_issueing_llm(prompt, document_string)\n",
    "      llm_dict = llm_output_parser(llm_response)\n",
    "      dict_to_json = {\n",
    "         \"url\": document['post_url'],\n",
    "         \"score\": document['post_score'],\n",
    "         \"numComents\": document['post_numComments'],\n",
    "         \"problem\": llm_dict[\"Problem\"],\n",
    "         \"causes\": llm_dict[\"Causes\"],\n",
    "         \"solutions\": llm_dict[\"Solutions\"]\n",
    "      }\n",
    "      count += 1\n",
    "      file_count += 1\n",
    "      write_json(f\"./llm_output/graph_data_{count}.json\", dict_to_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging successful. Written in: graph_data.json\n"
     ]
    }
   ],
   "source": [
    "# Code to be executed when graph_inference has completed its job\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Merging all the files in llm_output together\n",
    "folder_path = './llm_output'\n",
    "all_documents = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.json'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            document = json.load(file)\n",
    "            all_documents.append(document)\n",
    "\n",
    "output_file_path = 'graph_data.json'\n",
    "\n",
    "# Scrivi tutti i documenti in un unico file JSON, con indentazione per una formattazione corretta\n",
    "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    json.dump(all_documents, output_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f'Merging successful. Written in: {output_file_path}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
